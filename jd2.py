"""
äº¬ä¸œæ±½è½¦é›¶é…ä»¶é‡‡é›†å™¨ - è¯„åˆ†+è¯„è®ºæ•°å¢å¼ºç‰ˆ
æ–°å¢åŠŸèƒ½ï¼š
1. å•†å“è¯„åˆ†ï¼ˆå¦‚ 4.8åˆ†ï¼‰
2. è¯„è®ºæ•°é‡ï¼ˆå¦‚ 10ä¸‡+æ¡ï¼‰
3. å¤šé‡æå–ç­–ç•¥ç¡®ä¿æ•°æ®å®Œæ•´
"""

from DrissionPage import ChromiumPage, ChromiumOptions
import csv, time, os, re, random, ctypes
from ctypes import wintypes
from datetime import datetime
import json

class AutoPartsScraper:
    def __init__(self):
        co = ChromiumOptions()
        co.set_argument('--mute-audio')
        co.set_argument('--no-first-run')
        
        self.dp = ChromiumPage(addr_or_opts=co)
        print("="*60)
        print("âœ… æµè§ˆå™¨å·²å¯åŠ¨ (è¯„åˆ†+è¯„è®ºæ•°å¢å¼ºç‰ˆ)")
        print("="*60)

    def _get_true_desktop_path(self):
        try:
            buf = ctypes.create_unicode_buffer(wintypes.MAX_PATH)
            ctypes.windll.shell32.SHGetSpecialFolderPathW(None, buf, 0x0000, False)
            return buf.value
        except:
            return os.path.join(os.path.expanduser("~"), 'Desktop')

    def run(self, filename='å…³é”®è¯.txt', pages=15, output='æ±½è½¦é›¶é…ä»¶æ•°æ®.csv'):
        desktop_path = self._get_true_desktop_path()
        keywords_file = os.path.join(desktop_path, filename)
        output_file = os.path.join(desktop_path, output)

        if not os.path.exists(keywords_file):
            try:
                with open(keywords_file, 'w', encoding='utf-8') as f:
                    f.write("å…¨åˆæˆæœºæ²¹\nè¡Œè½¦è®°å½•ä»ª\nç±³å…¶æ—è½®èƒ")
            except: pass

        with open(keywords_file, 'r', encoding='utf-8') as f:
            keywords = [k.strip() for k in re.split(r'[,ï¼Œ\n]', f.read()) if k.strip()]
        
        if not keywords:
            print(f"âš ï¸  {filename} ä¸ºç©ºã€‚")
            return

        print(f"ğŸ“‹ é‡‡é›†ä»»åŠ¡: {len(keywords)}ä¸ªè¯ | ç›®æ ‡: {pages}é¡µ/è¯")
        
        self.dp.listen.start(['pc_search_searchWare', 'search', 'wareList'])
        
        total_count = 0
        
        for idx, kw in enumerate(keywords, 1):
            print(f"\n{'='*60}")
            print(f"ğŸ¯ [{idx}/{len(keywords)}] æ­£åœ¨é‡‡é›†: {kw}")
            print(f"{'='*60}")
            
            self.dp.listen.clear() 
            url = f'https://search.jd.com/Search?keyword={kw}&enc=utf-8&psort=3'
            self.dp.get(url)
            
            print("â³ ç­‰å¾…é¡µé¢...", end="")
            if not self.dp.ele('@data-sku', timeout=6):
                print(" è¶…æ—¶(å‡†å¤‡ç¡¬è§£æ)")
                self._handle_captcha()
            else:
                print(" å®Œæˆ")

            kw_products = []
            
            for page in range(1, pages + 1):
                print(f"\n   ğŸ“„ ç¬¬ {page} é¡µ", end="")
                self._human_scroll()
                
                raw_items = self._try_api()
                source = "API"
                
                if not raw_items:
                    raw_items = self.dp.eles('@data-sku')
                    raw_items = [item for item in raw_items if item.rect.size[1] > 10] 
                    source = "DOM"
                
                if not raw_items:
                    raw_items = self._try_regex_chunks()
                    source = "æºç ç¡¬æŠ "

                if len(raw_items) == 0:
                    print(f" -> {source}æ•è·(0ä¸ª) -> ğŸ›‘ æš‚åœï¼")
                    print("ğŸ‘‰ è¯·åœ¨æµè§ˆå™¨æ‰‹åŠ¨åˆ·æ–°æˆ–éªŒè¯ï¼Œè§£å†³åæŒ‰å›è½¦é‡è¯•...")
                    input()
                    raw_items = self.dp.eles('@data-sku')
                    source = "é‡è¯•DOM" if raw_items else "é‡è¯•å¤±è´¥"

                print(f" -> {source}æ•è·({len(raw_items)}ä¸ª)", end="")

                valid_items = []
                for i, item in enumerate(raw_items, 1):
                    p = self._parse_item_nuclear(item, kw, page, i)
                    if p: valid_items.append(p)

                print(f" -> âœ… å…¥åº“: {len(valid_items)}æ¡", end="")
                kw_products.extend(valid_items)

                if page < pages:
                    self.dp.listen.clear() 
                    if not self._next_page():
                        print(f" [åœæ­¢]", end="")
                        break
                    time.sleep(random.uniform(3, 5))
            
            if kw_products:
                self._save(kw_products, output_file)
                total_count += len(kw_products)
            
            self._remove_keyword_from_file(keywords_file, kw)
            if idx < len(keywords): time.sleep(random.randint(3, 5))
        
        print(f"\nğŸ‰ å®Œæˆï¼æ€»è®¡: {total_count}æ¡")
        self.dp.quit()

    def _human_scroll(self):
        """å¤šæ¬¡æ»šåŠ¨åŠ è½½æ›´å¤šå•†å“"""
        for _ in range(3):
            self.dp.scroll.to_bottom()
            time.sleep(1.5)
        self.dp.scroll.up(300)
        time.sleep(1)

    def _extract_comment_count(self, text):
        """æå–è¯„è®ºæ•°é‡"""
        if not text:
            return '0'
        
        # æ¨¡å¼1: 10ä¸‡+æ¡è¯„ä»· / 10ä¸‡+è¯„ä»·
        match = re.search(r'(\d+\.?\d*ä¸‡?\+?)(?:æ¡)?è¯„ä»·', text)
        if match:
            return match.group(1)
        
        # æ¨¡å¼2: å·²æœ‰10000äººè¯„ä»· / 10000äººè¯„ä»·
        match = re.search(r'(?:å·²æœ‰)?(\d+)äººè¯„ä»·', text)
        if match:
            return match.group(1)
        
        # æ¨¡å¼3: è¯„è®ºæ•° 10000
        match = re.search(r'è¯„è®º[æ•°é‡]?\s*[:ï¼š]?\s*(\d+)', text)
        if match:
            return match.group(1)
        
        # æ¨¡å¼4: å•ç‹¬çš„å¤§æ•°å­—
        match = re.search(r'(\d+\.?\d*ä¸‡\+?)', text)
        if match:
            num_str = match.group(1)
            if 'ä¸‡' in num_str:
                return num_str
        
        # æ¨¡å¼5: commentCount JSONå­—æ®µ
        match = re.search(r'commentCount["\']?\s*[:ï¼š]\s*["\']?(\d+)', text)
        if match:
            return match.group(1)
        
        return '0'

    def _extract_rating(self, text):
        """æå–å•†å“è¯„åˆ†ï¼ˆæ–°å¢ï¼‰"""
        if not text:
            return ''
        
        # æ¨¡å¼1: 4.8åˆ† / 4.8 / 98% å¥½è¯„
        match = re.search(r'(\d+\.?\d*)åˆ†', text)
        if match:
            return match.group(1)
        
        # æ¨¡å¼2: å¥½è¯„ç‡ 98%
        match = re.search(r'å¥½è¯„ç‡?\s*[:ï¼š]?\s*(\d+)%', text)
        if match:
            percent = int(match.group(1))
            # è½¬æ¢ä¸º5åˆ†åˆ¶è¯„åˆ†ï¼ˆè¿‘ä¼¼ï¼‰
            return str(round(percent * 5 / 100, 1))
        
        # æ¨¡å¼3: ç›´æ¥çš„æ•°å­—è¯„åˆ†ï¼ˆå¦‚APIè¿”å›ï¼‰
        match = re.search(r'(?:è¯„åˆ†|score)["\']?\s*[:ï¼š]\s*["\']?(\d+\.?\d*)', text, re.I)
        if match:
            return match.group(1)
        
        return ''

    def _parse_item_nuclear(self, item, kw, page, idx):
        """æ ¸å¼¹çº§è§£æå™¨ - å¢åŠ è¯„åˆ†å’Œè¯„è®ºæ•°"""
        try:
            res = {
                'é‡‡é›†æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'å…³é”®è¯': kw, 'é¡µç ': page,
                'SKU': '', 'æ ‡é¢˜': '', 'ä»·æ ¼': '', 
                'åº—é“º': '', 'è¯„åˆ†': '', 'è¯„è®ºæ•°': '', 'é“¾æ¥': ''
            }
            
            # --- 1. API æ¨¡å¼ ---
            if isinstance(item, dict) and not item.get('is_chunk'):
                res['SKU'] = str(item.get('skuId') or item.get('sku') or '')
                res['æ ‡é¢˜'] = (item.get('wname') or item.get('wareName') or 
                             item.get('title') or item.get('name') or '')
                res['ä»·æ ¼'] = str(item.get('jdPrice') or item.get('price') or '')
                res['åº—é“º'] = item.get('goodShop', {}).get('goodShopName') or item.get('shopName') or 'äº¬ä¸œ'
                
                # API è¯„è®ºæ•°
                comment_count = item.get('commentCount') or item.get('comments') or 0
                res['è¯„è®ºæ•°'] = str(comment_count)
                
                # API è¯„åˆ†
                score = item.get('score') or item.get('rating') or item.get('goodRate') or ''
                res['è¯„åˆ†'] = str(score) if score else ''

            # --- 2. DOM æ¨¡å¼ ---
            elif hasattr(item, 'ele'):
                res['SKU'] = item.attr('data-sku') or ''
                
                # [æ ‡é¢˜]
                img_ele = item.ele('tag:img', timeout=0.1)
                if img_ele: 
                    res['æ ‡é¢˜'] = img_ele.attr('alt') or ''
                
                if not res['æ ‡é¢˜']:
                    t_ele = item.ele('.p-name em', timeout=0.1)
                    if t_ele: 
                        res['æ ‡é¢˜'] = t_ele.text.strip()

                if not res['æ ‡é¢˜'] or len(res['æ ‡é¢˜']) < 3:
                    lines = item.text.split('\n')
                    valid_lines = [l for l in lines if len(l) > 8 and 'Â¥' not in l]
                    if valid_lines:
                        res['æ ‡é¢˜'] = max(valid_lines, key=len).strip()

                # [ä»·æ ¼]
                full_text = item.text
                p_match = re.search(r'[Â¥ï¿¥]\s*(\d+(\.\d+)?)', full_text)
                if p_match:
                    res['ä»·æ ¼'] = p_match.group(1)
                
                # [è¯„è®ºæ•°] - å¤šé‡ç­–ç•¥
                comment_ele = item.ele('.p-commit', timeout=0.1)
                if comment_ele:
                    res['è¯„è®ºæ•°'] = self._extract_comment_count(comment_ele.text)
                
                if not res['è¯„è®ºæ•°'] or res['è¯„è®ºæ•°'] == '0':
                    res['è¯„è®ºæ•°'] = self._extract_comment_count(full_text)
                
                if not res['è¯„è®ºæ•°'] or res['è¯„è®ºæ•°'] == '0':
                    comment_attr = item.attr('data-comment')
                    if comment_attr:
                        res['è¯„è®ºæ•°'] = comment_attr

                # [è¯„åˆ†] - æ–°å¢æå–é€»è¾‘
                # ç­–ç•¥1: æŸ¥æ‰¾è¯„åˆ†å…ƒç´ 
                rating_ele = item.ele('.p-score', timeout=0.1)
                if rating_ele:
                    res['è¯„åˆ†'] = self._extract_rating(rating_ele.text)
                
                # ç­–ç•¥2: ä»è¯„è®ºåŒºåŸŸæå–
                if not res['è¯„åˆ†'] and comment_ele:
                    res['è¯„åˆ†'] = self._extract_rating(comment_ele.text)
                
                # ç­–ç•¥3: ä»å…¨æ–‡æœ¬æå–
                if not res['è¯„åˆ†']:
                    res['è¯„åˆ†'] = self._extract_rating(full_text)
                
                # ç­–ç•¥4: æŸ¥æ‰¾ data-score å±æ€§
                if not res['è¯„åˆ†']:
                    score_attr = item.attr('data-score')
                    if score_attr:
                        res['è¯„åˆ†'] = score_attr

                # [åº—é“º]
                shop_ele = item.ele('.p-shop a', timeout=0.1)
                if shop_ele:
                    res['åº—é“º'] = shop_ele.text.strip()
                
                if not res['åº—é“º'] or res['åº—é“º'] == 'äº¬ä¸œ':
                    shop_div = item.ele('.p-shop', timeout=0.1)
                    if shop_div:
                        shop_text = shop_div.text.strip()
                        shop_text = re.sub(r'(è¿›åº—|å…³æ³¨|è‡ªè¥)', '', shop_text).strip()
                        if shop_text:
                            res['åº—é“º'] = shop_text
                
                if not res['åº—é“º'] or res['åº—é“º'] == 'äº¬ä¸œ':
                    shop_match = re.search(r'([^\s]+?(?:æ——èˆ°åº—|ä¸“è¥åº—|å®˜æ–¹åº—|è‡ªè¥|äº¬ä¸œ))', full_text)
                    if shop_match:
                        res['åº—é“º'] = shop_match.group(1)
                    else:
                        res['åº—é“º'] = 'äº¬ä¸œ'

            # --- 3. æºç ç¡¬æŠ æ¨¡å¼ ---
            elif isinstance(item, dict) and item.get('is_chunk'):
                chunk = item['chunk_html']
                sku_m = re.search(r'data-sku="(\d+)"', chunk)
                res['SKU'] = sku_m.group(1) if sku_m else ''
                
                t_match = re.search(r'title="([^"]+)"', chunk)
                if t_match: 
                    res['æ ‡é¢˜'] = t_match.group(1)
                else:
                    t_match = re.search(r'em[^>]*>([^<]+)</em>', chunk)
                    if t_match: 
                        res['æ ‡é¢˜'] = re.sub(r'<[^>]+>', '', t_match.group(1)).strip()
                
                p_match = re.search(r'[Â¥ï¿¥]\s*(\d+(\.\d+)?)', chunk)
                if p_match: 
                    res['ä»·æ ¼'] = p_match.group(1)
                
                # æºç æå–è¯„è®ºæ•°
                res['è¯„è®ºæ•°'] = self._extract_comment_count(chunk)
                
                # æºç æå–è¯„åˆ†
                res['è¯„åˆ†'] = self._extract_rating(chunk)
                
                # æºç æå–åº—é“º
                shop_match = re.search(r'data-shop[^>]*>([^<]+)</a>', chunk)
                if shop_match:
                    res['åº—é“º'] = shop_match.group(1).strip()
                else:
                    shop_match = re.search(r'([^\s]+?(?:æ——èˆ°åº—|ä¸“è¥åº—|å®˜æ–¹åº—|è‡ªè¥|äº¬ä¸œ))', chunk)
                    res['åº—é“º'] = shop_match.group(1) if shop_match else 'äº¬ä¸œ'

            # æ ¼å¼åŒ–
            if res['SKU']: 
                res['SKU'] = f"\t{res['SKU']}" 
            if res['ä»·æ ¼']: 
                res['ä»·æ ¼'] = re.sub(r'[^\d\.]', '', str(res['ä»·æ ¼']))
            
            res['é“¾æ¥'] = f"https://item.jd.com/{res['SKU'].strip()}.html"
            
            return res if res['SKU'].strip() else None
            
        except Exception as e:
            return None

    def _try_api(self):
        try:
            p = self.dp.listen.wait(['pc_search_searchWare', 'search', 'wareList'], timeout=2)
            if p: 
                return self._find_list_in_json(p.response.body)
        except: 
            return []

    def _find_list_in_json(self, data):
        if isinstance(data, dict):
            if 'skuId' in data and ('wname' in data or 'wareName' in data): 
                return [data]
            for key in ['wareList', 'wareInfo', 'searchm']:
                if key in data and isinstance(data[key], list): 
                    return data[key]
                if key in data and isinstance(data[key], dict): 
                    return self._find_list_in_json(data[key])
            for v in data.values():
                if isinstance(v, (dict, list)): 
                    r = self._find_list_in_json(v)
                    if r: 
                        return r
        elif isinstance(data, list):
            for i in data:
                r = self._find_list_in_json(i)
                if r: 
                    return r if isinstance(r, list) else [r]
        return []

    def _try_regex_chunks(self):
        try:
            html = self.dp.html
            chunks = []
            for match in re.finditer(r'data-sku="(\d+)"', html):
                start = match.start()
                chunk = html[max(0, start-200): min(len(html), start+1500)]
                chunks.append({'chunk_html': chunk, 'is_chunk': True})
            return chunks
        except: 
            return []

    def _next_page(self):
        try:
            btn = self.dp.ele('.pn-next', timeout=1) or self.dp.ele('text:ä¸‹ä¸€é¡µ', timeout=1)
            if btn and 'disabled' not in (btn.attr('class') or ''):
                btn.scroll.to_center()
                btn.click(by_js=True)
                return True
            return False
        except: 
            return False

    def _handle_captcha(self):
        if self.dp.ele('.JDJR-bigpic', timeout=1): 
            self.dp.wait.ele_hidden('.JDJR-bigpic', timeout=60)
        if 'passport.jd.com' in self.dp.url: 
            print("\nğŸš¨ è¯·ç™»å½•ï¼")
            while 'passport.jd.com' in self.dp.url: 
                time.sleep(2)

    def _save(self, products, filename):
        try:
            exist = os.path.exists(filename)
            with open(filename, 'a', encoding='utf-8-sig', newline='') as f:
                headers = list(products[0].keys())
                w = csv.DictWriter(f, fieldnames=headers)
                if not exist: 
                    w.writeheader()
                w.writerows(products)
        except: 
            pass

    def _remove_keyword_from_file(self, filename, kw):
        try:
            with open(filename, 'r', encoding='utf-8') as f: 
                lines = f.read().splitlines()
            lines = [l for l in lines if kw not in l and l.strip()]
            with open(filename, 'w', encoding='utf-8') as f: 
                f.write('\n'.join(lines))
        except: 
            pass

if __name__ == '__main__':
    AutoPartsScraper().run()