"""
äº¬ä¸œé‡‡é›†å™¨ - å¤šçº¿ç¨‹æé€Ÿç‰ˆ (é€‚é… AMD 9955HX)
ç‰¹æ€§ï¼š
1. [å¹¶å‘åŠ é€Ÿ] è¡¥å…¨åŠŸèƒ½é‡‡ç”¨ 32 çº¿ç¨‹å¹¶å‘ï¼Œå……åˆ†åˆ©ç”¨é«˜æ€§èƒ½CPUå’Œç½‘ç»œå¸¦å®½ã€‚
2. [æ•°æ®æ¸…æ´—] å¢å¼ºäº†å¯¹æ— æ•ˆæ•°æ®çš„è¿‡æ»¤ï¼Œå‡å°‘â€œç©ºæ•°æ®â€å…¥åº“ã€‚
3. [åŒæ¨¡è¿è¡Œ] 
   - æ¨¡å¼1: æµè§ˆå™¨é‡‡é›† (ç¨³å®šé˜²å°ï¼Œå•çº¿ç¨‹)
   - æ¨¡å¼2: æ¥å£è¡¥å…¨ (å¤šçº¿ç¨‹æé€Ÿï¼Œæ¯ç§’å¤„ç†å‡ åæ¡)
"""

from DrissionPage import ChromiumPage, ChromiumOptions
import csv, time, os, re, random, json, requests
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

# å…¨å±€é”ï¼Œé˜²æ­¢å¤šçº¿ç¨‹å†™å…¥CSVæ—¶å†²çª
csv_lock = Lock()

class JDFinalScraper:
    """ä¸»é‡‡é›†å™¨ï¼šè´Ÿè´£ç¿»é¡µæŠ“å–å•†å“åŸºç¡€ä¿¡æ¯ (SKU/æ ‡é¢˜/ä»·æ ¼)"""
    def __init__(self):
        co = ChromiumOptions()
        co.set_argument('--mute-audio')
        co.set_argument('--no-first-run')
        # ç§»é™¤è‡ªåŠ¨åŒ–ç‰¹å¾ï¼Œé™ä½è¢«æ£€æµ‹æ¦‚ç‡
        co.set_argument('--disable-blink-features=AutomationControlled')
        
        # å…¼å®¹æ–°æ—§ç‰ˆæœ¬ DrissionPage
        try:
            co.headless(False)
        except AttributeError:
            try: co.set_headless(False) 
            except: pass
        
        self.dp = ChromiumPage(addr_or_opts=co)
        print("="*60)
        print("âœ… ä¸»é‡‡é›†å™¨å·²å¯åŠ¨")
        print("="*60)

    def _get_desktop_path(self):
        return os.path.join(os.path.expanduser("~"), 'Desktop')

    def run(self, filename='å…³é”®è¯.txt', pages=10, output='æ±½è½¦é›¶é…ä»¶æ•°æ®.csv'):
        desktop_path = self._get_desktop_path()
        keywords_file = os.path.join(desktop_path, filename)
        output_file = os.path.join(desktop_path, output)

        if not os.path.exists(keywords_file):
            try:
                with open(keywords_file, 'w', encoding='utf-8') as f:
                    f.write("å…¨åˆæˆæœºæ²¹\nè¡Œè½¦è®°å½•ä»ª\nç±³å…¶æ—è½®èƒ")
            except: pass

        if not os.path.exists(keywords_file):
            print(f"âŒ æ‰¾ä¸åˆ°å…³é”®è¯æ–‡ä»¶")
            return

        with open(keywords_file, 'r', encoding='utf-8') as f:
            keywords = [k.strip() for k in re.split(r'[,ï¼Œ\n]', f.read()) if k.strip()]
        
        # å¯åŠ¨APIç›‘å¬
        self.dp.listen.start(['pc_search_searchWare', 'api.m.jd.com', 'search'])
        
        total_count = 0
        
        for idx, kw in enumerate(keywords, 1):
            print(f"\n{'='*60}")
            print(f"ğŸ¯ [{idx}/{len(keywords)}] æ­£åœ¨é‡‡é›†: {kw}")
            print(f"{'='*60}")
            
            self.dp.listen.clear() 
            url = f'https://search.jd.com/Search?keyword={kw}&enc=utf-8&psort=3'
            self.dp.get(url)
            
            # ç­‰å¾…åŠ è½½
            if not self.dp.ele('@data-sku', timeout=6):
                print("âš ï¸  ç­‰å¾…è¶…æ—¶ï¼Œå°è¯•æ‰‹åŠ¨éªŒè¯...")
                self._handle_captcha()

            kw_products = []
            
            for page in range(1, pages + 1):
                print(f"   ğŸ“„ ç¬¬{page}é¡µ", end=" ", flush=True)
                self._human_scroll()
                
                # --- å¤šé‡ç­–ç•¥ ---
                raw_items = self._try_api_targeted() # APIä¼˜å…ˆ
                source = "API"
                
                if not raw_items:
                    raw_items = self.dp.eles('@data-sku')
                    # è¿‡æ»¤æ— æ•ˆå…ƒç´ 
                    raw_items = [item for item in raw_items if item.rect.size[1] > 0] 
                    source = "DOM"
                
                if not raw_items:
                    raw_items = self._try_regex_chunks()
                    source = "æºç "

                if len(raw_items) == 0:
                    print(f" -> {source}(0) ğŸ›‘ æš‚åœ! è¯·åœ¨æµè§ˆå™¨æ‰‹åŠ¨æ“ä½œ...")
                    input("ğŸ‘‰ è§£å†³åæŒ‰å›è½¦...")
                    raw_items = self.dp.eles('@data-sku')
                    
                print(f"-> {source}({len(raw_items)})", end="")

                valid_items = []
                for i, item in enumerate(raw_items, 1):
                    p = self._parse_item_universal(item, kw, page, i)
                    # [æ•°æ®æ¸…æ´—] å¦‚æœSKUéƒ½æ²¡æœ‰ï¼Œç»å¯¹ä¸è¦
                    if p and p['SKU'].strip(): 
                        valid_items.append(p)

                print(f" -> âœ… {len(valid_items)}æ¡", end="")
                kw_products.extend(valid_items)

                if page < pages:
                    self.dp.listen.clear() 
                    if not self._next_page():
                        print(f" [æ— ä¸‹é¡µ]", end="")
                        break
                    time.sleep(random.uniform(2, 4))
                else:
                    print("")

            # å®æ—¶ä¿å­˜
            if kw_products:
                self._save(kw_products, output_file)
                total_count += len(kw_products)
            
            self._remove_keyword(keywords_file, kw)
            if idx < len(keywords): time.sleep(3)
        
        print(f"\nğŸ‰ é‡‡é›†ç»“æŸï¼æ€»è®¡: {total_count}æ¡")
        self.dp.quit()

    # --- è¾…åŠ©æ–¹æ³• (ä¿æŒåŸæœ‰çš„ç¨³å®šé€»è¾‘) ---
    def _human_scroll(self):
        self.dp.scroll.to_bottom(); time.sleep(1); self.dp.scroll.up(300)

    def _try_api_targeted(self):
        try:
            packets = self.dp.listen.steps(timeout=2)
            for packet in packets:
                if 'pc_search_searchWare' in packet.url or 'api.m.jd.com' in packet.url:
                    items = self._find_list_in_json(packet.response.body)
                    if items: return items
        except: pass
        return []

    def _find_list_in_json(self, data):
        if isinstance(data, dict):
            if 'skuId' in data and 'jdPrice' in data: return [data]
            for key in ['Paragraph', 'wareList', 'wareInfo', 'searchm', 'data', 'goodsList']:
                if key in data:
                    res = self._find_list_in_json(data[key])
                    if res: return res
            for v in data.values():
                if isinstance(v, (dict, list)): 
                    res = self._find_list_in_json(v)
                    if res: return res
        elif isinstance(data, list):
            if len(data) > 0 and isinstance(data[0], dict) and ('skuId' in data[0] or 'sku' in data[0]):
                return data
            for i in data:
                res = self._find_list_in_json(i)
                if res: return res
        return []

    def _parse_item_universal(self, item, kw, page, idx):
        try:
            res = {
                'é‡‡é›†æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'å…³é”®è¯': kw, 'é¡µç ': page,
                'SKU': '', 'æ ‡é¢˜': '', 'ä»·æ ¼': '', 
                'åº—é“º': '', 'é”€é‡': '', 'è¯„åˆ†': '', 'é“¾æ¥': ''
            }
            
            # APIæ¨¡å¼
            if isinstance(item, dict) and not item.get('is_chunk'):
                res['SKU'] = str(item.get('skuId') or item.get('sku') or '')
                res['æ ‡é¢˜'] = (item.get('wname') or item.get('wareName') or item.get('title') or '')
                res['ä»·æ ¼'] = str(item.get('jdPrice') or item.get('price') or '')
                res['åº—é“º'] = item.get('goodShop', {}).get('goodShopName') or item.get('shop_name') or ''
                res['é”€é‡'] = str(item.get('commentCount') or '0')

            # DOMæ¨¡å¼
            elif hasattr(item, 'ele'):
                res['SKU'] = item.attr('data-sku') or ''
                t_ele = item.ele('.p-name a', timeout=0.1)
                res['æ ‡é¢˜'] = t_ele.attr('title') or t_ele.text.strip() if t_ele else ''
                if not res['æ ‡é¢˜']: res['æ ‡é¢˜'] = item.ele('.p-name em').text.strip() if item.ele('.p-name em') else ''
                
                p_box = item.ele('.p-price', timeout=0.1)
                if p_box:
                    match = re.search(r'(\d+(\.\d+)?)', p_box.text)
                    if match: res['ä»·æ ¼'] = match.group(1)
                
                c_box = item.ele('.p-commit', timeout=0.1)
                if c_box:
                    match = re.search(r'(\d+[ä¸‡\+]*)', c_box.text)
                    if match: res['é”€é‡'] = match.group(1)
                
                s_ele = item.ele('.p-shop', timeout=0.1)
                res['åº—é“º'] = s_ele.text.strip() if s_ele else 'äº¬ä¸œ'

            # æºç æ¨¡å¼
            elif isinstance(item, dict) and item.get('is_chunk'):
                chunk = item['chunk_html']
                sku_m = re.search(r'data-sku="(\d+)"', chunk)
                res['SKU'] = sku_m.group(1) if sku_m else ''
                t_m = re.search(r'title="([^"]+)"', chunk)
                res['æ ‡é¢˜'] = t_m.group(1) if t_m else ''
                p_m = re.search(r'class="p-price".*?(\d+\.\d+)', chunk)
                res['ä»·æ ¼'] = p_m.group(1) if p_m else ''
                c_m = re.search(r'(\d+[ä¸‡\+]*)æ¡è¯„ä»·', chunk)
                res['é”€é‡'] = c_m.group(1) if c_m else '0'

            # æ ¼å¼åŒ–
            if res['SKU']: res['SKU'] = f"\t{res['SKU']}" 
            if res['ä»·æ ¼']: res['ä»·æ ¼'] = re.sub(r'[^\d\.]', '', str(res['ä»·æ ¼']))
            res['é“¾æ¥'] = f"https://item.jd.com/{res['SKU'].strip()}.html"
            
            return res if res['SKU'].strip() else None
        except: return None

    def _try_regex_chunks(self):
        try:
            html = self.dp.html
            chunks = []
            for match in re.finditer(r'data-sku="(\d+)"', html):
                start = match.start()
                chunk = html[max(0, start-200): min(len(html), start+1500)]
                chunks.append({'chunk_html': chunk, 'is_chunk': True})
            return chunks
        except: return []

    def _next_page(self):
        try:
            btn = self.dp.ele('.pn-next', timeout=1) or self.dp.ele('text:ä¸‹ä¸€é¡µ', timeout=1)
            if btn and 'disabled' not in (btn.attr('class') or ''):
                btn.scroll.to_center(); btn.click(by_js=True); return True
            return False
        except: return False

    def _handle_captcha(self):
        if self.dp.ele('.JDJR-bigpic', timeout=1) or 'passport.jd.com' in self.dp.url:
            print("\nğŸš¨ è¯·åœ¨æµè§ˆå™¨å®ŒæˆéªŒè¯...")
            while 'passport.jd.com' in self.dp.url: time.sleep(2)

    def _save(self, products, filename):
        try:
            exist = os.path.exists(filename)
            with open(filename, 'a', encoding='utf-8-sig', newline='') as f:
                headers = list(products[0].keys())
                w = csv.DictWriter(f, fieldnames=headers)
                if not exist: w.writeheader()
                w.writerows(products)
        except: pass

    def _remove_keyword(self, filename, kw):
        try:
            with open(filename, 'r', encoding='utf-8') as f: lines = f.read().splitlines()
            lines = [l for l in lines if kw not in l and l.strip()]
            with open(filename, 'w', encoding='utf-8') as f: f.write('\n'.join(lines))
        except: pass


# ---------------------------------------------------------------------
# å¤šçº¿ç¨‹è¡¥å…¨æ¨¡å— (åˆ©ç”¨ 9955HX é«˜æ€§èƒ½)
# ---------------------------------------------------------------------
class MultiThreadFiller:
    def __init__(self, workers=32):
        self.workers = workers # çº¿ç¨‹æ•°ï¼Œ9955HXå¯ä»¥è½»æ¾è·‘32-64çº¿ç¨‹
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://item.jd.com/'
        })

    def process_item(self, row):
        """å•ä¸ªå•†å“å¤„ç†å‡½æ•°"""
        sku = row.get('SKU', '').strip().replace('\t', '')
        if not sku: return None

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¡¥å…¨
        has_score = row.get('è¯„åˆ†') and row['è¯„åˆ†'].strip()
        has_sales = row.get('é”€é‡') and row['é”€é‡'].strip() and row['é”€é‡'] != '0'
        
        if has_score and has_sales:
            return None # ä¸éœ€è¦å¤„ç†

        # è¯·æ±‚æ•°æ®
        try:
            url = f"https://club.jd.com/comment/productCommentSummaries.action"
            params = {'referenceIds': sku}
            # è®¾ç½®è¾ƒçŸ­è¶…æ—¶ï¼Œåˆ©ç”¨å¤šçº¿ç¨‹å¿«é€Ÿè¿‡
            resp = self.session.get(url, params=params, timeout=5)
            data = resp.json()
            
            if 'CommentsCount' in data and data['CommentsCount']:
                item_data = data['CommentsCount'][0]
                
                # è¡¥å…¨è¯„åˆ†
                if not has_score:
                    rate = item_data.get('GoodRateShow', 0)
                    # å°†å¥½è¯„ç‡(100)è½¬æ¢ä¸º5åˆ†åˆ¶(5.0)
                    score = round(float(rate) * 5 / 100, 1)
                    row['è¯„åˆ†'] = str(score)
                
                # è¡¥å…¨é”€é‡
                if not has_sales:
                    c_str = item_data.get('CommentCountStr', '')
                    c_num = item_data.get('CommentCount', 0)
                    if c_str and c_str != '0':
                        row['é”€é‡'] = c_str.replace('+', '')
                    elif c_num:
                        row['é”€é‡'] = str(c_num)
                        
            return row # è¿”å›æ›´æ–°åçš„è¡Œ
        except:
            return None # å¤±è´¥è¿”å›ç©º

    def run(self, csv_file, output_file):
        print(f"\nğŸš€ å¯åŠ¨å¤šçº¿ç¨‹è¡¥å…¨ ({self.workers}çº¿ç¨‹)...")
        desktop = os.path.join(os.path.expanduser("~"), 'Desktop')
        input_path = os.path.join(desktop, csv_file)
        output_path = os.path.join(desktop, output_file)

        if not os.path.exists(input_path):
            print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
            return

        all_data = []
        with open(input_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            fieldnames = reader.fieldnames
            if 'è¯„åˆ†' not in fieldnames: fieldnames.append('è¯„åˆ†')
            if 'é”€é‡' not in fieldnames: fieldnames.append('é”€é‡')
            all_data = list(reader)

        print(f"ğŸ“‹ æ€»æ•°æ®: {len(all_data)} æ¡ï¼Œæ­£åœ¨åˆ†é…ä»»åŠ¡...")

        # å‡†å¤‡å·²å®Œæˆé›†åˆï¼Œé¿å…é‡å¤å†™å…¥
        processed_skus = set()
        if os.path.exists(output_path):
            with open(output_path, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    processed_skus.add(row.get('SKU', '').strip())
            print(f"ğŸ“š å†å²å·²å®Œæˆ: {len(processed_skus)} æ¡ (è·³è¿‡)")
        else:
            with open(output_path, 'w', encoding='utf-8-sig', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()

        # ç­›é€‰ä»»åŠ¡
        tasks = []
        for row in all_data:
            sku = row.get('SKU', '').strip()
            if sku not in processed_skus:
                tasks.append(row)

        if not tasks:
            print("âœ… æ‰€æœ‰æ•°æ®å·²å®Œæˆ")
            return

        print(f"âš¡ å¼€å§‹å¹¶å‘å¤„ç† {len(tasks)} æ¡ä»»åŠ¡...")
        
        count = 0
        success = 0
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=self.workers) as executor:
            futures = {executor.submit(self.process_item, row): row for row in tasks}
            
            for future in as_completed(futures):
                count += 1
                result_row = future.result()
                
                # åŸå§‹è¡Œï¼ˆç”¨äºå¤±è´¥æ—¶ä¹Ÿä¿å­˜ï¼Œé˜²æ­¢æ•°æ®ä¸¢å¤±ï¼‰
                original_row = futures[future]
                row_to_save = result_row if result_row else original_row
                
                # çº¿ç¨‹å®‰å…¨å†™å…¥
                with csv_lock:
                    try:
                        with open(output_path, 'a', encoding='utf-8-sig', newline='') as f:
                            writer = csv.DictWriter(f, fieldnames=fieldnames)
                            # ç¡®ä¿SKUæ ¼å¼
                            if not row_to_save['SKU'].startswith('\t'):
                                row_to_save['SKU'] = f"\t{row_to_save['SKU']}"
                            writer.writerow(row_to_save)
                    except: pass
                
                if result_row: success += 1
                
                # è¿›åº¦æ¡
                if count % 50 == 0:
                    print(f"\rğŸš€ è¿›åº¦: {count}/{len(tasks)} | æˆåŠŸè¡¥å…¨: {success}", end="")

        print(f"\n\nğŸ‰ å…¨éƒ¨å®Œæˆï¼æˆåŠŸè¡¥å…¨: {success} æ¡")
        print(f"ğŸ’¾ ç»“æœä¿å­˜è‡³: {output_file}")


if __name__ == '__main__':
    print("1. é‡‡é›†æ•°æ® (å•çº¿ç¨‹ç¨³å®š)")
    print("2. æé€Ÿè¡¥å…¨æ•°æ® (å¤šçº¿ç¨‹ï¼Œ9955HXç«åŠ›å…¨å¼€)")
    choice = input("è¯·é€‰æ‹©: ").strip()
    
    if choice == '1':
        JDFinalScraper().run()
    else:
        csv_file = input("è¾“å…¥æ–‡ä»¶å [é»˜è®¤: æ±½è½¦é›¶é…ä»¶æ•°æ®.csv]: ").strip() or 'æ±½è½¦é›¶é…ä»¶æ•°æ®.csv'
        # 32çº¿ç¨‹å¯¹äº9955HXæ¥è¯´éå¸¸è½»æ¾ï¼Œæ—¢èƒ½è·‘æ»¡ç½‘é€Ÿåˆä¸ä¼šå¡æ­»
        MultiThreadFiller(workers=32).run(
            csv_file=csv_file, 
            output_file=csv_file.replace('.csv', '_å®Œæ•´ç‰ˆ.csv')
        )