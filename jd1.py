"""
äº¬ä¸œæ±½è½¦é›¶é…ä»¶é‡‡é›†å™¨ - å…¨èƒ½è¡¥å…¨ç‰ˆ
ç‰¹æ€§ï¼š
1. [æš´åŠ›è¡¥å…¨] æ ‡é¢˜å’Œä»·æ ¼ä¸ºç©ºæ—¶ï¼Œå¯ç”¨å…¨æ–‡æœ¬æ‰«æå’Œé“¾æ¥åŒ¹é…ï¼Œç¡®ä¿ä¸æ¼æ•°æ®ã€‚
2. [APIå¢å¼º] å…¼å®¹æ›´å¤šAPIå­—æ®µå‘½åï¼Œé˜²æ­¢æ¥å£æ”¹ç‰ˆå¯¼è‡´å–ä¸åˆ°å€¼ã€‚
3. [æ ¸å¿ƒé€»è¾‘] ä¼˜å…ˆçº§ï¼šAPI > DOM(å¢å¼º) > æºç æ­£åˆ™(å¢å¼º)ã€‚
"""

from DrissionPage import ChromiumPage, ChromiumOptions
import csv, time, os, re, random, ctypes
from ctypes import wintypes
from datetime import datetime
import json

class AutoPartsScraper:
    def __init__(self):
        co = ChromiumOptions()
        # å¼€å¯å›¾ç‰‡åŠ è½½ä»¥æé«˜å®‰å…¨æ€§ï¼Œé˜²æ­¢è¢«è¯†åˆ«ä¸ºæœºå™¨äºº
        # co.set_argument('--blink-settings=imagesEnabled=false') 
        co.set_argument('--mute-audio')
        co.set_argument('--no-first-run')
        
        self.dp = ChromiumPage(addr_or_opts=co)
        print("="*60)
        print("âœ… æµè§ˆå™¨å·²å¯åŠ¨ (æ•°æ®è¡¥å…¨æ¨¡å¼)")
        print("="*60)

    def _get_true_desktop_path(self):
        try:
            buf = ctypes.create_unicode_buffer(wintypes.MAX_PATH)
            ctypes.windll.shell32.SHGetSpecialFolderPathW(None, buf, 0x0000, False)
            return buf.value
        except:
            return os.path.join(os.path.expanduser("~"), 'Desktop')

    def run(self, filename='å…³é”®è¯.txt', pages=15, output='æ±½è½¦é›¶é…ä»¶æ•°æ®.csv'):
        desktop_path = self._get_true_desktop_path()
        keywords_file = os.path.join(desktop_path, filename)
        output_file = os.path.join(desktop_path, output)

        if not os.path.exists(keywords_file):
            try:
                with open(keywords_file, 'w', encoding='utf-8') as f:
                    f.write("å…¨åˆæˆæœºæ²¹\nè¡Œè½¦è®°å½•ä»ª\nç±³å…¶æ—è½®èƒ")
                print(f"ğŸ“ å·²åœ¨æ¡Œé¢åˆ›å»ºæµ‹è¯•æ–‡ä»¶: {filename}")
            except: pass

        with open(keywords_file, 'r', encoding='utf-8') as f:
            keywords = [k.strip() for k in re.split(r'[,ï¼Œ\n]', f.read()) if k.strip()]
        
        if not keywords:
            print(f"âš ï¸  {filename} ä¸ºç©ºï¼Œè¯·æ·»åŠ å…³é”®è¯ã€‚")
            return

        print(f"ğŸ“‹ é‡‡é›†ä»»åŠ¡:")
        print(f"   å…³é”®è¯æ•°: {len(keywords)} | ç›®æ ‡é¡µæ•°: {pages}")
        print(f"   æ’åºæ¨¡å¼: é”€é‡/è¯„ä»·æ•°é™åº (psort=3)")
        print(f"   è¾“å‡ºæ–‡ä»¶: {output}")
        
        self.dp.listen.start(['pc_search_searchWare', 'search', 'wareList'])
        
        total_count = 0
        
        for idx, kw in enumerate(keywords, 1):
            print(f"\n{'='*60}")
            print(f"ğŸ¯ [{idx}/{len(keywords)}] æ­£åœ¨é‡‡é›†: {kw}")
            print(f"{'='*60}")
            
            self.dp.listen.clear() 
            url = f'https://search.jd.com/Search?keyword={kw}&enc=utf-8&psort=3'
            self.dp.get(url)
            
            print("â³ ç­‰å¾…é¡µé¢åŠ è½½...", end="")
            if not self.dp.ele('@data-sku', timeout=6):
                print(" è¶…æ—¶(å‡†å¤‡å¯ç”¨ç¡¬è§£æ)")
                self._handle_captcha()
            else:
                print(" å®Œæˆ")

            kw_products = []
            
            for page in range(1, pages + 1):
                print(f"\n   ğŸ“„ ç¬¬ {page} é¡µ", end="")
                self._human_scroll()
                
                # --- ç­–ç•¥æ‰§è¡Œ ---
                raw_items = self._try_api()
                source = "API"
                
                if not raw_items:
                    raw_items = self.dp.eles('@data-sku')
                    raw_items = [item for item in raw_items if item.rect.size[1] > 0] 
                    source = "DOM"
                
                if not raw_items:
                    raw_items = self._try_regex_chunks()
                    source = "æºç ç¡¬æŠ "

                if len(raw_items) == 0:
                    print(f" -> {source}æ•è·(0ä¸ª)")
                    print("\nğŸ›‘ ã€å¼‚å¸¸ã€‘æ£€æµ‹åˆ°0æ•°æ®ï¼å¯èƒ½æ˜¯éªŒè¯ç æˆ–æœªç™»å½•ã€‚")
                    print("ğŸ‘‰ è¯·åœ¨æµè§ˆå™¨æ‰‹åŠ¨å¤„ç†ï¼Œç„¶åæŒ‰ã€å›è½¦ã€‘é‡è¯•...")
                    input()
                    print("ğŸ”„ é‡è¯•...", end="")
                    raw_items = self._try_api()
                    if not raw_items: raw_items = self.dp.eles('@data-sku')
                    source = "é‡è¯•" if raw_items else "é‡è¯•å¤±è´¥"

                print(f" -> {source}æ•è·({len(raw_items)}ä¸ª)", end="")

                valid_items = []
                for i, item in enumerate(raw_items, 1):
                    p = self._parse_item_robust(item, kw, page, i)
                    if p: valid_items.append(p)

                print(f" -> âœ… å…¥åº“: {len(valid_items)}æ¡", end="")
                kw_products.extend(valid_items)

                if page < pages:
                    self.dp.listen.clear() 
                    if not self._next_page():
                        print(f" [åœæ­¢ç¿»é¡µ]", end="")
                        break
                    time.sleep(random.uniform(3, 5))
            
            if kw_products:
                self._save(kw_products, output_file)
                total_count += len(kw_products)
            
            self._remove_keyword_from_file(keywords_file, kw)
            if idx < len(keywords):
                print("\nâ˜• ä¼‘æ¯ 5 ç§’...")
                time.sleep(5)
        
        print(f"\nğŸ‰ å…¨éƒ¨é‡‡é›†å®Œæˆï¼ç´¯è®¡è·å–: {total_count} æ¡æ•°æ®")
        self.dp.quit()

    # ================= æ ¸å¿ƒå¢å¼ºé€»è¾‘ =================

    def _human_scroll(self):
        self.dp.scroll.to_bottom()
        time.sleep(1.5)
        self.dp.scroll.up(500)
        time.sleep(0.5)
        self.dp.scroll.up(300)

    def _parse_item_robust(self, item, kw, page, idx):
        """[å¢å¼ºç‰ˆ] è§£æå™¨ï¼šç©·å°½ä¸€åˆ‡æ‰‹æ®µè·å–æ ‡é¢˜å’Œä»·æ ¼"""
        try:
            res = {
                'é‡‡é›†æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'å…³é”®è¯': kw, 'é¡µç ': page,
                'SKU': '', 'æ ‡é¢˜': '', 'ä»·æ ¼': '', 
                'åº—é“º': '', 'è¯„è®ºæ•°': '', 'é“¾æ¥': ''
            }
            raw_comment = '0'
            
            # --- 1. API æ¨¡å¼ (ä¼˜å…ˆ) ---
            if isinstance(item, dict) and not item.get('is_chunk'):
                res['SKU'] = str(item.get('skuId') or item.get('sku') or '')
                # å°è¯•æ‰€æœ‰å¯èƒ½çš„æ ‡é¢˜å­—æ®µ
                res['æ ‡é¢˜'] = (item.get('wname') or item.get('wareName') or 
                             item.get('title') or item.get('name') or '')
                # å°è¯•æ‰€æœ‰å¯èƒ½çš„ä»·æ ¼å­—æ®µ
                res['ä»·æ ¼'] = str(item.get('jdPrice') or item.get('price') or '')
                res['åº—é“º'] = item.get('goodShop', {}).get('goodShopName') or item.get('shopName') or ''
                raw_comment = str(item.get('commentCount') or '0')

            # --- 2. DOM æ¨¡å¼ (æš´åŠ›æŸ¥æ‰¾) ---
            elif hasattr(item, 'ele'):
                res['SKU'] = item.attr('data-sku') or ''
                
                # [æ ‡é¢˜ä¿®å¤] å¤šé‡å…œåº•æŸ¥æ‰¾
                t_ele = item.ele('.p-name a', timeout=0.1) # 1. æ‰¾æ ‡å‡†æ ‡é¢˜é“¾æ¥
                if t_ele:
                    # ä¼˜å…ˆå– title å±æ€§ï¼ˆé€šå¸¸æ˜¯å…¨åï¼‰ï¼Œæ²¡æœ‰å–æ–‡æœ¬
                    res['æ ‡é¢˜'] = t_ele.attr('title') or t_ele.text.strip()
                
                # 2. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œæ‰¾ä»»ä½•å«SKUçš„é“¾æ¥
                if not res['æ ‡é¢˜']:
                    t_ele = item.ele(f'a[href*="{res["SKU"]}"]', timeout=0.1)
                    if t_ele: res['æ ‡é¢˜'] = t_ele.attr('title') or t_ele.text.strip()
                
                # 3. å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œæ‰¾å­—æ•°æœ€å¤šçš„æ–‡æœ¬è¡Œï¼ˆç»ˆæå…œåº•ï¼‰
                if not res['æ ‡é¢˜']:
                    lines = item.text.split('\n')
                    # è¿‡æ»¤æ‰çº¯æ•°å­—æˆ–ä»·æ ¼è¡Œ
                    valid_lines = [l for l in lines if len(l) > 5 and not re.match(r'^[Â¥ï¿¥0-9\.]+$', l.strip())]
                    if valid_lines:
                        res['æ ‡é¢˜'] = max(valid_lines, key=len).strip()

                # [ä»·æ ¼] ä¼˜å…ˆæ‰¾ .p-price
                p_ele = item.ele('.p-price', timeout=0.1)
                if p_ele:
                    # æå–ä»»ä½•çœ‹èµ·æ¥åƒä»·æ ¼çš„æ•°å­— (æ”¯æŒ Â¥299.00 æˆ– 299)
                    price_match = re.search(r'[Â¥ï¿¥]?\s*(\d+(\.\d+)?)', p_ele.text)
                    res['ä»·æ ¼'] = price_match.group(1) if price_match else ''
                
                # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œæš´åŠ›æ‰«ææ•´ä¸ªå¡ç‰‡æ–‡æœ¬
                if not res['ä»·æ ¼']:
                    full_text = item.text
                    price_match = re.search(r'[Â¥ï¿¥]\s*(\d+(\.\d+)?)', full_text)
                    res['ä»·æ ¼'] = price_match.group(1) if price_match else ''

                s_ele = item.ele('.p-shop', timeout=0.1)
                res['åº—é“º'] = s_ele.text.strip() if s_ele else 'äº¬ä¸œ'
                
                c_ele = item.ele('.p-commit', timeout=0.1)
                raw_comment = c_ele.text.strip() if c_ele else '0'

            # --- 3. æºç ç¡¬æŠ æ¨¡å¼ ---
            elif isinstance(item, dict) and item.get('is_chunk'):
                chunk = item['chunk_html']
                sku_m = re.search(r'data-sku="(\d+)"', chunk)
                res['SKU'] = sku_m.group(1) if sku_m else ''
                
                # [æ ‡é¢˜ä¿®å¤] å¢åŠ å¯¹ title="..." å±æ€§çš„åŒ¹é…
                t_match_attr = re.search(r'class="p-name".*?title="([^"]+)"', chunk, re.DOTALL)
                if t_match_attr:
                    res['æ ‡é¢˜'] = t_match_attr.group(1).strip()
                else:
                    # å¤‡é€‰ï¼šåŒ¹é… em æ ‡ç­¾
                    t_match = re.search(r'class="p-name".*?em[^>]*>([^<]+)</em>', chunk, re.DOTALL)
                    if t_match: res['æ ‡é¢˜'] = t_match.group(1).strip()
                
                # ä»·æ ¼ï¼šæ”¾å®½æ­£åˆ™ï¼Œå¯»æ‰¾ Â¥ åé¢çš„æ•°å­—
                p_match = re.search(r'[Â¥ï¿¥](?:<[^>]+>)*\s*(\d+(?:\.\d+)?)', chunk)
                if p_match:
                    res['ä»·æ ¼'] = p_match.group(1)
                
                raw_comment = "0+"

            # [ä¿®å¤] Excel ç§‘å­¦è®¡æ•°æ³•
            if res['SKU']: res['SKU'] = f"\t{res['SKU']}" 
            res['è¯„è®ºæ•°'] = raw_comment
            res['é“¾æ¥'] = f"https://item.jd.com/{res['SKU'].strip()}.html"
            
            # åªæœ‰å½“SKUå­˜åœ¨æ—¶æ‰è¿”å›
            return res if res['SKU'].strip() else None
            
        except: return None

    def _try_api(self):
        try:
            p = self.dp.listen.wait(['pc_search_searchWare', 'search', 'wareList'], timeout=2)
            if p: return self._find_list_in_json(p.response.body)
        except: return []

    def _find_list_in_json(self, data):
        if isinstance(data, dict):
            if 'skuId' in data and ('wname' in data or 'wareName' in data): return [data]
            for key in ['wareList', 'wareInfo', 'searchm']:
                if key in data and isinstance(data[key], list): return data[key]
                if key in data and isinstance(data[key], dict): return self._find_list_in_json(data[key])
            for v in data.values():
                if isinstance(v, (dict, list)): 
                    r = self._find_list_in_json(v)
                    if r: return r
        elif isinstance(data, list):
            for i in data:
                r = self._find_list_in_json(i)
                if r: return r if isinstance(r, list) else [r]
        return []

    def _try_regex_chunks(self):
        try:
            html = self.dp.html
            chunks = []
            for match in re.finditer(r'data-sku="(\d+)"', html):
                start = match.start()
                chunk = html[max(0, start-200): min(len(html), start+1500)]
                chunks.append({'chunk_html': chunk, 'is_chunk': True})
            return chunks
        except: return []

    def _next_page(self):
        try:
            btn = self.dp.ele('.pn-next', timeout=1) or self.dp.ele('text:ä¸‹ä¸€é¡µ', timeout=1)
            if btn and 'disabled' not in (btn.attr('class') or ''):
                btn.scroll.to_center(); btn.click(by_js=True); return True
            return False
        except: return False

    def _handle_captcha(self):
        if self.dp.ele('.JDJR-bigpic', timeout=1): self.dp.wait.ele_hidden('.JDJR-bigpic', timeout=60)
        if 'passport.jd.com' in self.dp.url: 
            print("\nğŸš¨ è¯·ç™»å½•ï¼"); 
            while 'passport.jd.com' in self.dp.url: time.sleep(2)

    def _save(self, products, filename):
        try:
            exist = os.path.exists(filename)
            with open(filename, 'a', encoding='utf-8-sig', newline='') as f:
                headers = list(products[0].keys())
                w = csv.DictWriter(f, fieldnames=headers)
                if not exist: w.writeheader()
                w.writerows(products)
        except: pass

    def _remove_keyword_from_file(self, filename, kw):
        try:
            with open(filename, 'r', encoding='utf-8') as f: lines = f.read().splitlines()
            lines = [l for l in lines if kw not in l and l.strip()]
            with open(filename, 'w', encoding='utf-8') as f: f.write('\n'.join(lines))
        except: pass

if __name__ == '__main__':
    AutoPartsScraper().run()